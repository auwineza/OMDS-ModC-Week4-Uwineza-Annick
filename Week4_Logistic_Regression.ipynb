{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Logistic Regression & Feature Scaling (CKD)\n",
    "**Integrated Capstone — Week 4 Notebook**  \n",
    "\n",
    "This notebook creates a **binary CKD label from GFR** and compares **Logistic Regression** with and without **feature scaling**.\n",
    "\n",
    "We will:\n",
    "- Build a binary target: **CKD_flag = 1 if GFR < 60, else 0**\n",
    "- Compare pipelines (no scaling vs. StandardScaler)\n",
    "- Tune regularization via **LogisticRegressionCV**\n",
    "- Evaluate with **Accuracy, Precision, Recall, F1, ROC-AUC**\n",
    "- Plot ROC curves and show a confusion matrix\n",
    "- Inspect top positive/negative coefficients (log-odds)\n",
    "\n",
    "**Dataset:** `Chronic_Kidney_Dsease_data.csv`  \n",
    "**Original target:** `GFR`  → **Derived label:** `CKD_flag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Configuration\n",
    "DATA_PATH = \"Chronic_Kidney_Dsease_data.csv\"   # keep relative so peers can run from GitHub\n",
    "GFR_COL = \"GFR\"\n",
    "CKD_THRESHOLD = 60.0  # GFR < 60 → CKD present\n",
    "TEST_SIZE = 0.2\n",
    "CV_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# If class imbalance is strong, using class_weight='balanced' can help recall at some cost to precision\n",
    "USE_CLASS_WEIGHT = True  # set False if you don't want automatic balancing\n",
    "\n",
    "# Results folder\n",
    "OUT_DIR = \"week4_outputs_ckd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, confusion_matrix, RocCurveDisplay\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "Path(OUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load data & create binary label (CKD_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "assert GFR_COL in df.columns, f\"Column '{GFR_COL}' not found. Available: {df.columns.tolist()[:25]}…\"\n",
    "\n",
    "# Create binary label: 1 = CKD present (GFR < 60), 0 = no CKD\n",
    "df[\"CKD_flag\"] = (df[GFR_COL].astype(float) < CKD_THRESHOLD).astype(int)\n",
    "\n",
    "print(\"Loaded:\", df.shape)\n",
    "display(df.head(3))\n",
    "print(\"\\nClass balance (CKD_flag):\\n\", df[\"CKD_flag\"].value_counts(normalize=True).rename(lambda x: f\"class_{x}\"))\n",
    "\n",
    "# Define X, y\n",
    "y = df[\"CKD_flag\"]\n",
    "X = df.drop(columns=[\"CKD_flag\"])  # keep original GFR and other cols as features for now\n",
    "\n",
    "# Identify types\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(f\"Numerical: {len(num_cols)} | Categorical: {len(cat_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train / Test split (stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \"| Test:\", X_test.shape)\n",
    "print(\"Train class balance:\", y_train.mean().round(3), \"(1=CKD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Preprocessing pipelines (with and without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared imputers/encoders\n",
    "num_impute = SimpleImputer(strategy=\"median\")\n",
    "cat_impute = SimpleImputer(strategy=\"most_frequent\")\n",
    "onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "# A) No-scaling pipeline\n",
    "pre_no_scale = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imputer\", num_impute)]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imputer\", cat_impute), (\"onehot\", onehot)]), cat_cols)\n",
    "])\n",
    "\n",
    "# B) With StandardScaler for numeric\n",
    "pre_scaled = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imputer\", num_impute), (\"scaler\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imputer\", cat_impute), (\"onehot\", onehot)]), cat_cols)\n",
    "])\n",
    "\n",
    "class_weight = \"balanced\" if USE_CLASS_WEIGHT else None\n",
    "\n", 
    "# CV-based logistic regression (strongly recommended over fixed C)\n",
    "def make_logregcv_pipeline(preprocessor):\n",
    "    # liblinear handles l1/l2 on smaller data; saga good for large/sparse; we'll use liblinear for robustness\n",
    "    return Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", LogisticRegressionCV(\n",
    "            Cs=np.logspace(-3, 3, 10),\n",
    "            cv=CV_FOLDS,\n",
    "            penalty=\"l2\",\n",
    "            solver=\"liblinear\",\n",
    "            class_weight=class_weight,\n",
    "            scoring=\"roc_auc\",\n",
    "            max_iter=1000,\n",
    "            n_jobs=None\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "pipe_no_scale = make_logregcv_pipeline(pre_no_scale)\n",
    "pipe_scaled   = make_logregcv_pipeline(pre_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Fit models & evaluate (Accuracy, Precision, Recall, F1, ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(name, pipe, X_tr, X_te, y_tr, y_te):\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    # Predictions\n",
    "    yhat_tr = pipe.predict(X_tr)\n",
    "    yhat_te = pipe.predict(X_te)\n",
    "    # Probabilities for ROC-AUC\n",
    "    proba_tr = pipe.predict_proba(X_tr)[:, 1]\n",
    "    proba_te = pipe.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    def metrics(y_true, y_pred, proba):\n",
    "        return dict(\n",
    "            Accuracy=accuracy_score(y_true, y_pred),\n",
    "            Precision=precision_score(y_true, y_pred, zero_division=0),\n",
    "            Recall=recall_score(y_true, y_pred, zero_division=0),\n",
    "            F1=f1_score(y_true, y_pred, zero_division=0),\n",
    "            ROC_AUC=roc_auc_score(y_true, proba)\n",
    "        )\n",
    "\n",
    "    tr = metrics(y_tr, yhat_tr, proba_tr)\n",
    "    te = metrics(y_te, yhat_te, proba_te)\n",
    "\n",
    "    row = {\n",
    "        \"Model\": name,\n",
    "        **{f\"train_{k}\": v for k, v in tr.items()},\n",
    "        **{f\"test_{k}\": v for k, v in te.items()}\n",
    "    }\n",
    "    return row, pipe\n",
    "\n",
    "rows = []\n",
    "row_ns, fitted_ns = evaluate_classifier(\"LogRegCV — No Scaling\", pipe_no_scale, X_train, X_test, y_train, y_test)\n",
    "rows.append(row_ns)\n",
    "\n",
    "row_sc, fitted_sc = evaluate_classifier(\"LogRegCV — Scaled\", pipe_scaled, X_train, X_test, y_train, y_test)\n",
    "rows.append(row_sc)\n",
    "\n",
    "results = pd.DataFrame(rows)\n",
    "display(results)\n",
    "\n",
    "# Save\n",
    "results.to_csv(Path(OUT_DIR) / \"week4_classification_results.csv\", index=False)\n",
    "print(\"Saved:\", (Path(OUT_DIR) / \"week4_classification_results.csv\").resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Classification report & confusion matrix (best model on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best by test ROC-AUC; tie-breaker test F1\n",
    "best_idx = results[[\"test_ROC_AUC\", \"test_F1\"]].values.argmax(axis=0)[0]\n",
    "best_row = results.iloc[results[\"test_ROC_AUC\"].idxmax()]\n",
    "best_name = best_row[\"Model\"]\n",
    "best_pipe = fitted_sc if \"Scaled\" in best_name else fitted_ns\n",
    "\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "y_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Best model: {best_name}\\n\")\n",
    "print(\"Classification Report (Test):\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix (Test):\\n\", cm)\n",
    "\n",
    "# Simple ROC curve\n",
    "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "plt.title(f\"ROC Curve — {best_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Coefficient inspection (log-odds, top positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(preprocessor, num_cols, cat_cols):\n",
    "    # numeric\n",
    "    num_names = preprocessor.named_transformers_[\"num\"].get_feature_names_out(num_cols) \\\n",
    "        if hasattr(preprocessor.named_transformers_[\"num\"], \"get_feature_names_out\") else np.array(num_cols)\n",
    "    # categorical (from OneHot)\n",
    "    if len(cat_cols) > 0 and hasattr(preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"], \"get_feature_names_out\"):\n",
    "        cat_names = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(cat_cols)\n",
    "        return np.concatenate([num_names, cat_names])\n",
    "    return num_names\n",
    "\n",
    "# Use the best fitted pipeline\n",
    "pre = best_pipe.named_steps[\"preprocess\"]\n",
    "model = best_pipe.named_steps[\"model\"]\n",
    "feat_names = get_feature_names(pre, num_cols, cat_cols)\n",
    "coefs = model.coef_.ravel()\n",
    "\n",
    "coef_df = pd.DataFrame({\"feature\": feat_names, \"coef\": coefs})\n",
    "coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
    "coef_df = coef_df.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "display(coef_df.head(20))\n",
    "\n",
    "coef_df.to_csv(Path(OUT_DIR) / \"week4_best_model_coefficients.csv\", index=False)\n",
    "print(\"Saved:\", (Path(OUT_DIR) / \"week4_best_model_coefficients.csv\").resolve())\n",
    "\n",
    "# Plot top +/- coefficients\n",
    "topk = coef_df.head(15)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.barh(topk[\"feature\"], topk[\"coef\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"Top Coefficients (log-odds) — {best_name}\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Notes on interpretation\n",
    "- Coefficients are **log-odds**: positive values increase odds of CKD (label=1), negative decrease odds.\n",
    "- With scaling enabled, numeric features are on comparable scales, which often improves convergence and interpretability.\n",
    "- If the class distribution is imbalanced, `class_weight='balanced'` boosts recall for the minority class, sometimes reducing precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Takeaways (fill in before submitting)\n",
    "- **Which pipeline performed best (by test ROC-AUC / F1)?** Did scaling help?\n",
    "- **Class imbalance:** Did `class_weight='balanced'` change Recall vs Precision?\n",
    "- **Most influential features:** Which top coefficients made clinical sense?\n",
    "- **Confusion matrix & ROC:** Any threshold tuning needed (precision–recall trade-off)?\n",
    "- **Next steps:** Calibrated probabilities, threshold tuning, or trying non-linear classifiers (trees/GBM/SVM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
